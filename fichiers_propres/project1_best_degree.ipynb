{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"data/train.csv\" # download train data and supply path\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our training set into 4 different groups depending on the value of the `PRI_jet_num` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_num = 0\n",
    "y0, tX0, ids0 = jet_num_split(jet_num, y, tX, ids)\n",
    "\n",
    "jet_num = 1\n",
    "y1, tX1, ids1 = jet_num_split(jet_num, y, tX, ids)\n",
    "\n",
    "jet_num = 2\n",
    "y2, tX2, ids2 = jet_num_split(jet_num, y, tX, ids)\n",
    "\n",
    "jet_num = 3\n",
    "y3, tX3, ids3 = jet_num_split(jet_num, y, tX, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best degrees for the polynomial feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import *\n",
    "from implementation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the polynomial feature expansion up to `max_degree` using  `k_fold` cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 3\n",
    "k_fold = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced training set for `PRI_jet_num=0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation for degree optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 0.5439418893456567 TE 0.13598547233641417 DEG 1\n",
      "TR 0.7002540516395086 TE 0.17506351290987715 DEG 2\n",
      "TR 0.9999715104838616 TE 0.2499928776209654 DEG 3\n"
     ]
    }
   ],
   "source": [
    "expansion_degrees0, tX0_expanded = degree_cross_validation(y0, tX0, k_fold, max_degree, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with feature expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, loss0 = least_squares(y0, tX0_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced training set for `PRI_jet_num=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation for degree optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 0.7224975464428748 TE 0.1806243866107187 DEG 1\n",
      "TR 0.6818464229734459 TE 0.17046160574336147 DEG 2\n",
      "TR 0.7972286811401599 TE 0.19930717028503997 DEG 3\n"
     ]
    }
   ],
   "source": [
    "expansion_degrees1, tX1_expanded = degree_cross_validation(y1, tX1, 4, max_degree, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with feature expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, loss1 = least_squares(y1, tX1_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced training set for `PRI_jet_num=2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation for degree optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 0.7000639175036556 TE 0.1750159793759139 DEG 1\n",
      "TR 0.7849021872484996 TE 0.1962255468121249 DEG 2\n",
      "TR 0.999936731781592 TE 0.249984182945398 DEG 3\n"
     ]
    }
   ],
   "source": [
    "expansion_degrees2, tX2_expanded = degree_cross_validation(y2, tX2, 4, max_degree, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with feature expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2, loss2 = least_squares(y2, tX2_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced training set for `PRI_jet_num=3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation for degree optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR 0.7017729049125367 TE 0.17544322622813419 DEG 1\n",
      "TR 0.7696390312907369 TE 0.1924097578226842 DEG 2\n",
      "TR 0.9998968592065258 TE 0.24997421480163146 DEG 3\n"
     ]
    }
   ],
   "source": [
    "expansion_degrees3, tX3_expanded = degree_cross_validation(y3, tX3, 4, max_degree, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning with feature expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3, loss3 = least_squares(y3, tX3_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = \"data/test.csv\" #download test data and supply path\n",
    "y_pred, tX_pred, ids_pred = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and feature expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_num = 0\n",
    "y0_pred, tX0_pred, ids0_pred = jet_num_split(jet_num, y_pred, tX_pred, ids_pred)\n",
    "\n",
    "jet_num = 1\n",
    "y1_pred, tX1_pred, ids1_pred = jet_num_split(jet_num, y_pred, tX_pred, ids_pred)\n",
    "\n",
    "jet_num = 2\n",
    "y2_pred, tX2_pred, ids2_pred = jet_num_split(jet_num, y_pred, tX_pred, ids_pred)\n",
    "\n",
    "jet_num = 3\n",
    "y3_pred, tX3_pred, ids3_pred = jet_num_split(jet_num, y_pred, tX_pred, ids_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRI_jet_num = 0\n",
    "tX0_pred_expanded = np.c_[tX0_pred, np.ones(tX0_pred.shape[0])]\n",
    "for feature_index in range(expansion_degrees0.shape[0]-1, -1, -1):\n",
    "    tX0_pred_expanded = build_poly_index(tX0_pred_expanded, expansion_degrees0[feature_index], feature_index)\n",
    "\n",
    "\n",
    "# PRI_jet_num = 1\n",
    "tX1_pred_expanded = np.c_[tX1_pred, np.ones(tX1_pred.shape[0])]\n",
    "for feature_index in range(expansion_degrees1.shape[0]-1, -1, -1):\n",
    "    tX1_pred_expanded = build_poly_index(tX1_pred_expanded, expansion_degrees1[feature_index], feature_index)\n",
    "\n",
    "\n",
    "# PRI_jet_num = 2\n",
    "tX2_pred_expanded = np.c_[tX2_pred, np.ones(tX2_pred.shape[0])]\n",
    "for feature_index in range(expansion_degrees2.shape[0]-1, -1, -1):\n",
    "    tX2_pred_expanded = build_poly_index(tX2_pred_expanded, expansion_degrees2[feature_index], feature_index)\n",
    "\n",
    "\n",
    "# PRI_jet_num = 3\n",
    "tX3_pred_expanded = np.c_[tX3_pred, np.ones(tX3_pred.shape[0])]\n",
    "for feature_index in range(expansion_degrees3.shape[0]-1, -1, -1):\n",
    "    tX3_pred_expanded = build_poly_index(tX3_pred_expanded, expansion_degrees3[feature_index], feature_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'results.csv' #name of output file for submission\n",
    "\n",
    "y0_pred = predict_labels(w0, tX0_pred_expanded)\n",
    "y1_pred = predict_labels(w1, tX1_pred_expanded)\n",
    "y2_pred = predict_labels(w2, tX2_pred_expanded)\n",
    "y3_pred = predict_labels(w3, tX3_pred_expanded)\n",
    "\n",
    "y_pred_sep = np.concatenate([y0_pred, y1_pred, y2_pred, y3_pred])\n",
    "ids_te_sep = np.concatenate([ids0_pred, ids1_pred, ids2_pred, ids3_pred])\n",
    "\n",
    "create_csv_submission(ids_te_sep, y_pred_sep, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
